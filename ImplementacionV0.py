# -*- coding: utf-8 -*-
"""BTC-Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ss-LIB18EcWluLeqYN0VBh3Lcb4431oi

# Configuraci√≥n del Entorno
Configura el entorno de ejecuci√≥n con todas las dependencias necesarias para la implementaci√≥n.
- Instala PyTorch y librer√≠as auxiliares
- Configura el dispositivo (GPU/CPU) para acelerar el entrenamiento
- Importa todos los m√≥dulos que se usar√°n en el proyecto
- `torch.__version__`: Versi√≥n de PyTorch (puedes cambiar a una espec√≠fica)
- `device`: Cambia entre "cuda" (GPU) o "cpu" seg√∫n disponibilidad
- Prepara el contexto para las celdas siguientes
"""

# BITCOIN TRADING CON M-DQN (PyTorch)
## Implementaci√≥n basada en el paper de Scientific Reports
## pip install pandas numpy matplotlib torch
## pip install gymnasium  # Opcional, para estilo

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import random
from collections import deque

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""# Preprocesamiento de Datos
Prepara los datos hist√≥ricos de Bitcoin para el entrenamiento del modelo.
- Carga datos CSV de precios de Bitcoin
- Realiza limpieza y transformaci√≥n de datos
- Genera caracter√≠sticas t√©cnicas (returns, volatilidad)

### Par√°metros:
- `window_size=24`: Tama√±o de ventana para caracter√≠sticas hist√≥ricas
- `noise_level=0.1`: Nivel de ruido en sentimientos simulados
- Columnas del CSV: Ajustar seg√∫n formato de tus datos

***üõëNotas y Apuntes a resolver***:
- Simula datos de sentimiento de Twitter si no se tienen reales
- Permite trabajar sin datos reales de Twitter (modo simulaci√≥n)
- Las caracter√≠sticas generadas deben tener sentido financiero
"""

class BitcoinPricePreprocessor:
    def __init__(self, window_size=24):
        self.window_size = window_size

    def load_data(self, csv_path, skip_first_line=True):
        """Carga TU CSV exacto de CryptoDataDownload"""
        print(f"Cargando: {csv_path}")

        # Saltar primera l√≠nea descriptiva
        df = pd.read_csv(csv_path, skiprows=1) if skip_first_line else pd.read_csv(csv_path)

        # Renombrar columnas
        df.columns = ['unix', 'date', 'symbol', 'open', 'high', 'low', 'close',
                      'volume_btc', 'volume_usd']

        # Ordenar cronol√≥gicamente (tus datos est√°n descendentes)
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date')
        df.set_index('date', inplace=True)

        # Convertir a num√©rico
        numeric_cols = ['open', 'high', 'low', 'close', 'volume_btc', 'volume_usd']
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        # Filtrar precios inv√°lidos
        df = df[df['close'] > 0]

        print(f"{len(df)} filas cargadas | {df.index[0].date()} a {df.index[-1].date()}")
        return df

    def create_features(self, df, feature_level='medium'):
        """Crea caracter√≠sticas t√©cnicas seg√∫n nivel de complejidad"""
        print(f"Creando caracter√≠sticas ({feature_level} level)...")

        # B√ÅSICAS (siempre necesarias)
        df['returns'] = df['close'].pct_change()
        df['volatility_6h'] = df['returns'].rolling(6).std()
        df['volatility_24h'] = df['returns'].rolling(24).std()
        df['momentum_6h'] = df['close'] / df['close'].shift(6) - 1

        if feature_level in ['medium', 'high']:
            # MEDIUM: caracter√≠sticas adicionales del paper
            df['price_range'] = (df['high'] - df['low']) / df['close']
            df['volume_ratio'] = df['volume_usd'] / df['volume_usd'].rolling(24).mean()
            df['hour_of_day'] = df.index.hour

            # SMA y cruces
            df['sma_12'] = df['close'].rolling(12).mean()
            df['sma_24'] = df['close'].rolling(24).mean()
            df['sma_cross'] = df['sma_12'] - df['sma_24']

        if feature_level == 'high':
            # HIGH: caracter√≠sticas avanzadas
            df['rsi'] = self._calculate_rsi(df['close'], period=14)
            df['bb_upper'], df['bb_lower'] = self._bollinger_bands(df['close'])
            df['atr'] = self._average_true_range(df)

        df.dropna(inplace=True)
        print(f"{len(df.columns)} caracter√≠sticas creadas")
        return df

    def _calculate_rsi(self, prices, period=14):
        """Calcula RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))

    def _bollinger_bands(self, prices, window=20, num_std=2):
        """Calcula Bollinger Bands"""
        sma = prices.rolling(window).mean()
        std = prices.rolling(window).std()
        upper = sma + (std * num_std)
        lower = sma - (std * num_std)
        return upper, lower

    def _average_true_range(self, df, period=14):
        """Calcula Average True Range"""
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = ranges.max(axis=1)
        return true_range.rolling(period).mean()

# Funci√≥n de carga de datos
def load_btc_data_for_training(file_path='data/raw/bitcoin_price/btc_hourly.csv',
                               sentiment_path='data/raw/twitter/engtweetsbtc_clean.csv',
                               feature_level='medium'):
    """Carga y prepara datos para entrenamiento"""
    preprocessor = BitcoinPricePreprocessor()

    # Cargar y procesar BTC 
    df = preprocessor.load_data(file_path)
    df = preprocessor.create_features(df, feature_level=feature_level)

    # Estad√≠sticas
    print(f"  Resumen:")
    print(f"  Filas: {len(df)}")
    print(f"  Columnas: {len(df.columns)}")
    print(f"  Precio inicial: ${df['close'].iloc[0]:.2f}")
    print(f"  Precio final: ${df['close'].iloc[-1]:.2f}")
    print(f"  Returns promedio: {df['returns'].mean():.6f}")
    print(f"  Volatilidad promedio: {df['volatility_24h'].mean():.6f}")

    return df, preprocessor

# URGENTE: ENCONTRAR BANCO REAL DE DATOS Y ACTUALIZAR CLASE
class TwitterSentimentSimulator:
    def __init__(self, seed=42):
        np.random.seed(seed)

    def generate_sentiment(self, price_data, noise_level=0.1):
        """Genera sentimientos correlacionados con precios"""
        returns = price_data['returns'].values
        # Sentimiento basado en momentum
        sentiment = np.tanh(returns * 10) + np.random.normal(0, noise_level, len(returns))
        return np.clip(sentiment, -1, 1)

"""# Entorno de Trading
Crea un simulador de mercado de Bitcoin donde el agente puede operar.
- Simula un broker con balance, posiciones y comisiones
- Define estados observables (precio, posici√≥n, balance)
- Implementa acciones (comprar, vender, mantener)
- Calcula recompensas basadas en profit y penalizaciones
- Entorno controlado para entrenamiento RL
- Permite probar estrategias sin riesgo real
- Implementa restricciones del mundo real (comisiones)

### Pr√°metros:
- `initial_balance=10000`: Capital inicial para trading
- `fee=0.0015`: Comisi√≥n por transacci√≥n (0.15% como paper)
- Acciones disponibles: Puedes a√±adir m√°s (ej: short selling)
- Las penalizaciones por inactividad ayudan a evitar overfitting. Penaliza con -0.001 por inactividad o acci√≥n "HOLD"

***üõëNotas y Apuntes a resolver***:
- El entorno sigue la interfaz est√°ndar de Gymnasium
"""

class BitcoinTradingEnv:
    """Entorno personalizado para trading de Bitcoin"""

    def __init__(self, price_data, sentiment_data=None, initial_balance=10000):
        self.price_data = price_data
        self.sentiment_data = sentiment_data
        self.initial_balance = initial_balance
        self.reset()

    def reset(self):
        self.current_step = 0
        self.balance = self.initial_balance
        self.bitcoin_held = 0
        self.total_profit = 0
        self.trades = []
        self.current_price = self.price_data['close'].iloc[self.current_step]

        return self._get_state()

    def _get_state(self):
        # Estado: [balance_ratio, bitcoin_ratio, returns, sentiment]
        balance_ratio = self.balance / (self.initial_balance + 1e-8)
        bitcoin_value = self.bitcoin_held * self.current_price
        total = self.balance + bitcoin_value
        bitcoin_ratio = bitcoin_value / (total + 1e-8)

        state = [balance_ratio, bitcoin_ratio]

        # A√±adir datos hist√≥ricos si est√°n disponibles
        if self.current_step > 0:
            prev_price = self.price_data['close'].iloc[self.current_step - 1]
            state.append((self.current_price - prev_price) / prev_price)
        else:
            state.append(0)

        # A√±adir sentimiento si existe
        if self.sentiment_data is not None:
            state.append(self.sentiment_data.iloc[self.current_step])
        else:
            state.append(0)

        return np.array(state, dtype=np.float32)

    def step(self, action):
        # action: 0=hold, 1=buy, 2=sell
        self.current_price = self.price_data['close'].iloc[self.current_step]

        reward = 0
        fee = 0.0015  # 0.15% como en el paper

        if action == 1:  # BUY
            if self.balance > 0:
                cost = self.current_price * (1 + fee)
                max_bitcoin = self.balance / cost
                self.bitcoin_held += max_bitcoin
                self.balance = 0
                self.trades.append(('buy', self.current_step, self.current_price))

        elif action == 2:  # SELL
            if self.bitcoin_held > 0:
                revenue = self.bitcoin_held * self.current_price * (1 - fee)
                self.balance += revenue
                profit = revenue - (self.bitcoin_held * self.current_price)
                reward += profit
                self.total_profit += profit
                self.bitcoin_held = 0
                self.trades.append(('sell', self.current_step, self.current_price))

        # Penalizaci√≥n por inactividad (como en el paper)
        if action == 0:
            if len(self.trades) > 0 and self.trades[-1][0] == 'hold':
                reward -= 0.001
            self.trades.append(('hold', self.current_step, self.current_price))

        # Siguiente paso
        self.current_step += 1
        next_state = self._get_state()
        done = self.current_step >= len(self.price_data) - 1

        return next_state, reward, done, {}

"""# Arquitectura de Redes Neuronales
Define las arquitecturas de las redes DQN para cada m√≥dulo del sistema.
- Trade-DQN: Toma precios ‚Üí recomienda acciones (buy/hold/sell)
- Predictive-DQN: Toma precio+sentimiento ‚Üí predice cambio porcentual
- Main-DQN: Combina outputs anteriores ‚Üí decisi√≥n final

### Par√°metros:
- Dimensiones de capas (64, 32, etc.)
- Funciones de activaci√≥n (ReLU, LeakyReLU)
- N√∫mero de neuronas por capa
- Dropout rates para regularizaci√≥n

***üõëNotas y Apuntes a resolver***:
- Las arquitecturas siguen las descritas en el paper
- Predictive-DQN tiene 20001 salidas para -100 a +100 con 2 decimales
"""

# 4.1 DQN base
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

# 4.2 Trade-DQN (especializado)
class TradeDQN(DQN):
    def __init__(self):
        super().__init__(state_dim=3, action_dim=3)  # precio + features b√°sicas

# 4.3 Predictive-DQN
class PredictiveDQN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(2, 64)  # precio + sentimiento
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 20001)  # -100 a +100 con 2 decimales

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

"""# Agente DQN
Implementa el algoritmo de aprendizaje por refuerzo con t√©cnicas de estabilizaci√≥n.
- Experience Replay: Almacena y muestrea experiencias pasadas
- Target Network: Red separada para calcular targets estables
- Œµ-greedy con decaimiento
- Optimizaci√≥n con Adam y MSE loss
- Evita correlaciones en datos secuenciales

### Par√°metros
- `lr=0.001`: Tasa de aprendizaje
- `gamma=0.95`: Factor de descuento de recompensas futuras
- `epsilon_decay=0.995`: Velocidad de reducci√≥n de exploraci√≥n
- `memory_size=10000`: Tama√±o del buffer de experiencias

***üõëNotas y Apuntes a resolver***:
- Experience replay es clave para convergencia estable
"""

class DQNAgent:
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.95):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995

        self.model = DQN(state_dim, action_dim).to(device)
        self.target_model = DQN(state_dim, action_dim).to(device)
        self.target_model.load_state_dict(self.model.state_dict())

        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)
        self.memory = deque(maxlen=10000)
        self.batch_size = 64

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_dim)

        state = torch.FloatTensor(state).unsqueeze(0).to(device)
        with torch.no_grad():
            q_values = self.model(state)
        return q_values.argmax().item()

    def replay(self):
        if len(self.memory) < self.batch_size:
            return

        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.FloatTensor(states).to(device)
        actions = torch.LongTensor(actions).unsqueeze(1).to(device)
        rewards = torch.FloatTensor(rewards).to(device)
        next_states = torch.FloatTensor(next_states).to(device)
        dones = torch.FloatTensor(dones).to(device)

        # Q-values actuales
        current_q = self.model(states).gather(1, actions).squeeze()

        # Q-values del siguiente estado
        with torch.no_grad():
            next_q = self.target_model(next_states).max(1)[0]
            target_q = rewards + (1 - dones) * self.gamma * next_q

        # Loss
        loss = F.mse_loss(current_q, target_q)

        # Optimizaci√≥n
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Decaimiento de epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def update_target(self):
        self.target_model.load_state_dict(self.model.state_dict())

"""# Entrenamiento del sistema MDQN
Orquesta el ciclo completo de entrenamiento del modelo.
- Loop sobre episodios de entrenamiento
- Interacci√≥n agente-entorno paso a paso
- Registro de m√©tricas de rendimiento (Contra overfitting)

### Par√°metros
- `episodes=100`: N√∫mero de episodios de entrenamiento
- `target_update_freq=10`: Frecuencia de actualizaci√≥n de target network
- Batch size: Tama√±o del minibatch para training
"""

def train_mdqn(env, agent, episodes=100, target_update_freq=10):
    rewards_history = []
    profit_history = []

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0

        for step in range(len(env.price_data) - 1):
            action = agent.act(state)
            next_state, reward, done, _ = env.step(action)

            agent.remember(state, action, reward, next_state, done)
            agent.replay()

            state = next_state
            total_reward += reward

            if done:
                break

        # Actualizar target network
        if episode % target_update_freq == 0:
            agent.update_target()

        rewards_history.append(total_reward)
        profit_history.append(env.total_profit)

        print(f"Episode {episode+1}/{episodes} | "
              f"Reward: {total_reward:.2f} | "
              f"Profit: {env.total_profit:.2f} | "
              f"Epsilon: {agent.epsilon:.3f}")

    return rewards_history, profit_history

"""# Evaluaci√≥n y M√©tricas Financieras
Eval√∫a el rendimiento del modelo entrenado con m√©tricas financieras est√°ndar.
- ROI (Return on Investment): Ganancia porcentual sobre capital
- Sharpe Ratio: Retorno ajustado por riesgo
- N√∫mero de trades: Frecuencia de operaciones
- Balance final: Capital total al final del periodo
- Cuantifica el √©xito de la estrategia
- Permite comparar con segunda versi√≥n del modelo

### Par√°metros
- Per√≠odo de evaluaci√≥n (train/test split)
- M√©tricas adicionales (Sortino Ratio, Max Drawdown)
- Umbrales de riesgo para Sharpe Ratio

***üõëNotas y Apuntes a resolver***:
- ROI alto ‚â† buena estrategia (puede ser muy riesgosa)
- Sharpe Ratio > 1 generalmente considerado bueno
- Las m√©tricas deben evaluarse en conjunto
"""

def calculate_metrics(env, agent, test_data):
    # ROI
    initial = env.initial_balance
    final = env.balance + env.bitcoin_held * env.current_price
    roi = ((final - initial) / initial) * 100

    # Sharpe Ratio (simplificado)
    returns = []
    for i in range(1, len(test_data)):
        ret = (test_data['close'].iloc[i] - test_data['close'].iloc[i-1]) / test_data['close'].iloc[i-1]
        returns.append(ret)

    sharpe_ratio = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(365*24)

    return {
        'ROI': roi,
        'Sharpe Ratio': sharpe_ratio,
        'Final Balance': final,
        'Number of Trades': len([t for t in env.trades if t[0] in ['buy', 'sell']])
    }

"""# Ejecuci√≥n Principal
Flujo completo del sistema con datos simulados/reales.
1. Carga y preprocesa datos de Bitcoin
2. Entrena Trade-DQN
3. Eval√∫a en conjunto de test
4. Visualiza resultados

### Par√°metros
- Ruta del archivo CSV de datos
- Capital inicial para simulaci√≥n
- Divisi√≥n train/test (80/20 por defecto)
- Hiperpar√°metros de entrenamiento

***üõëNotas y Apuntes a resolver***:
- Ajustar par√°metros seg√∫n tus datos disponibles: btc_hourly .csv
- Guardar modelos entrenados para uso en entorno real
"""

# Cargar datos
preprocessor = BitcoinPricePreprocessor()
df = preprocessor.load_data('btc_hourly.csv')
df = preprocessor.create_features(df)

# Simular sentimientos (o cargar reales)
sentiment_sim = TwitterSentimentSimulator()
sentiments = sentiment_sim.generate_sentiment(df)

# Dividir datos
train_size = int(len(df) * 0.8)
train_data = df.iloc[:train_size]
test_data = df.iloc[train_size:]

# Entrenar Trade-DQN
print("Entrenando TradeDQN")
env = BitcoinTradingEnv(train_data, initial_balance=10000)
agent = DQNAgent(state_dim=4, action_dim=3)  # ajustar seg√∫n estado
rewards, profits = train_mdqn(env, agent, episodes=50)

# Visualizar resultados
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(rewards)
plt.title('Recompensas por episodio')
plt.subplot(1, 2, 2)
plt.plot(profits)
plt.title('Profit acumulado')
plt.show()

# Evaluar
test_env = BitcoinTradingEnv(test_data, initial_balance=10000)
metrics = calculate_metrics(test_env, agent, test_data)
print("M√©tricas finales:")
for key, value in metrics.items():
    print(f"{key}: {value:.4f}")